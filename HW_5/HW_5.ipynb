{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PmFITaKlbO0",
        "colab_type": "text"
      },
      "source": [
        "# **General concepts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKAGPhc8l5n3",
        "colab_type": "text"
      },
      "source": [
        "## What is Artificial Intelligence?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz8Men07yY2d",
        "colab_type": "text"
      },
      "source": [
        "Artificial Intelligence can be traced back to man's first inception of automatic, mechanical tools or machines.\n",
        "\n",
        "Ancient records detail that with the invention of machines, humanity naturally began to relate them to the likeness of living beings for there weren't any non-living things that could act in such unique, defined, and particular ways. As machines have evolved, so too has humanity's interest in Artificial Intelligence.\n",
        "\n",
        "At first, machines were constructed in human and animalia form. We often regarded such creations with intrigue and as our mechanical prowess improved, we were able to make them increasingly more life-like and more advanced.\n",
        "\n",
        "However, it wasn't until the invention of the computer that Artificial Intelligence was really thrust to the forefront of human culture. With the advent of the computer, complexity and machine intelligence was increasingly easier to develop and thus Artificial Intelligence became more advanced than ever before. Serious concerns started to arise that these computers could, potentially, become more intelligent than we human beings given it is able to self-learn in a generalized way.\n",
        "\n",
        "This perfectly matched the 80s culture in the west and from it, social and entertainment value was gained. Movies, games, books, all forms of media and entertainment harped on the idea. From this massive public interest, funding was thrust into professional, academic research of Artificial Intelligence and its possibilities. This brings us to the computational domain of Artificial Intelligence.\n",
        "\n",
        "Artificial Intelligence can refer to many things. It can refer to machines and how they do complex tasks that only lifeforms have been able to do. It can refer to robotic beings that have consciousness and could supplant humanity. Or it can refer to a computational field of algorithms.\n",
        "\n",
        "The computational field of Artificial Intelligence was created near the 1950s. It started with simple, automated algorithms to play chess, the famous Turing Test, and a bot that could play checkers which then developed into advanced Bayesian algorithms, industrial assembly line robots, semantic nets, path finding algorithms (dijkstra's, iterative deepening, DFS, best-first search), perceptrons, constraint propagation (forward checking, k-consistency, DPLL and Resolution), Huffman-Clowes Labeling, and an untold amount of other algorithms.\n",
        "\n",
        "These rule-based and search-focused algorithms are now, oftentimes referred to as Classical Artificial Intelligence. After these algorithms, there were some massive breakthroughs which have reformed the Artificial Intelligence field we know today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_ez46JZyROS",
        "colab_type": "text"
      },
      "source": [
        "## What is Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9R1ptNLElxb",
        "colab_type": "text"
      },
      "source": [
        "The most significant advancement in modern Artificial Intelligence is Machine Learning. Machine Learning has brought us subdomains such as Natural Language Processing (NLP) and Deep Learning. Revolutionary and groundbreaking techniques which allow us to compute in a way that has never been seen before.\n",
        "\n",
        "One of the biggest pitfalls to Rule-based Artificial Intelligence is subjectivity. Humans are able to reason and compute subjective things, but with Rule-based Artificial Intelligence, unless every component is explicitly defined and written, it is difficult for it to succeed, so subjective matters are nigh on impossible to compute.\n",
        "\n",
        "Machine Learning on the otherhand, is able to compute subjective things. Using statistics and probability, Machine Learning can take a large amount of abstract data, learn from it, reinforce itself, and become more atuned to said data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUiw_CqyUf8",
        "colab_type": "text"
      },
      "source": [
        "## What is Deep Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gDUJbBHH63f",
        "colab_type": "text"
      },
      "source": [
        "A subset of Machine Learning, Deep Learning is also able to learn as we give it more data, but Deep Learning relies on Neural Networks. Neural Networks enable learning through accuracy. If a network's accuracy is low, the network can automatically adjust itself to account for the failure whereas in Machine Learning the programmer may have to manually modify the code to ensure learning. Thus, Deep Learning is a completely automated learning process similar to how living beings learn.\n",
        "\n",
        "It is most well known for its hand in Natural Language Processing and Image Classification, taking major problems with little success and turning them around with extraordinary results. Not to mention the other applications that affect everyday lives such as learning user preferences to curate experiences on the internet, spell checking and prediction, and translation. Deep Learning is, without a doubt, one of the greatest breakthroughs of our time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5g7xMVAleLh",
        "colab_type": "text"
      },
      "source": [
        "# **Basic concepts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bult6eUTL7dI",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpcsEfrvQ4aj",
        "colab_type": "text"
      },
      "source": [
        "Linear Regression is a way of approximating trends in data linearly. Using this approximation on certain types of data, results in the ability to take advantage of patterns and predict trends.\n",
        "\n",
        "It starts with the linear formula $y = mx+b$. If you plot multiple points of data along the $x$ axis, a line of best fit can be approximated. Henceforth, if that data's trend can be represented accurately with the linear formula, it can be predicted given an $x$ value.\n",
        "\n",
        "Thus we obtain the formula $\\hat{y}= b + w_1x_1$. Where there must be a bias $b$, some slope aka weight $w_1$ and some $x$ value $x_1$. This can be generalized to $\\hat{y}= b + \\sum_{i=1}^{n} w_ix_i$ for multiple slopes/weights and input values.\n",
        "\n",
        "Weights are different than slopes such that they indicate they aren't perfect, can change, and are related to Neural Networks. The bias is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ph8cwsUL-MP",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g1OsvOKY7qX",
        "colab_type": "text"
      },
      "source": [
        "With Linear Regression, we approximated trends *linearly*. Logistic Regression is similar in which it also approximates trends, but it approximates them along the sigmoid function.\n",
        "\n",
        "The sigmoid function is defined as $\\sigma(z)=\\frac{1}{1+e^{-z}}$. Notice it takes $z$ as a parameter. $z$ here is actually $\\hat{y}$. Thus the sigmoid function can be written as $\\sigma(\\hat{y})=\\frac{1}{1+e^{-\\hat{y}}}$.\n",
        "\n",
        "This means Logistic Regression is approximated with the sigmoid function along $\\hat{y}$.\n",
        "\n",
        "Logistic Regression is useful for classification problems where the answer is categorical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5ZTVWFwML75",
        "colab_type": "text"
      },
      "source": [
        "## Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ggOU7lcd8ey",
        "colab_type": "text"
      },
      "source": [
        "A gradient reflects a function's growth. It is used in Gradient Descent in Machine Learning to help the computer find the direction of greatest increase. The minimum or maximum of the gradient gives us the most optimal weights and by approaching the minimum or maximum, we can optimize for them. By finding the minimum or maximum, we have achieved the best possible result for our configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4WOILl0jdEf",
        "colab_type": "text"
      },
      "source": [
        "![img](https://i.imgur.com/YGOGzJb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTewanC_MNzn",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iubghOyBfu8c",
        "colab_type": "text"
      },
      "source": [
        "In a Neural Network, we wish to find the most optimal weights. Aka the weights that fit the data in the best possible way and make our predictions as accurate as possible. We do this by updating the weights continuously. Each time the network iterates in training, accuracy is noted and the weights are adjusted accordingly. But it needs to be determined *how much* to update the weights. We don't want to overshoot the optimal value, but if we undershoot too much, it could take us an eternity to find the optimal value. So, how do we solve this problem? How do we adjust the weights?\n",
        "\n",
        "The answer: Gradient Descent. Using Gradient Descent, we can take into account how off we are from the optimal weight value. If our weight values are far off, then we adjust the weights a lot. If our weight values are very close, then we adjust the weight values just a bit. Every time we iterate through our data, we adjust our weights down a gradient for our function. This adjustment amount is also called the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvq7iYX_kbnp",
        "colab_type": "text"
      },
      "source": [
        "![image](https://i.imgur.com/O1zGIto.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBUWo5Tohy6s",
        "colab_type": "text"
      },
      "source": [
        "### Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osItEX6LiLYL",
        "colab_type": "text"
      },
      "source": [
        "**Stochastic Gradient Descent**: We randomly select a single piece of data, perform a gradient descent on that data, and update our weights accordingly.\n",
        "\n",
        "**Batch Gradient Descent**: We use the entire batch of data to perform a gradient descent and then update our weights accordingly.\n",
        "\n",
        "**Mini-Batch Gradient Descent**: We use a portion of our batch of data to perform a gradient descent and then update our weights accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJuTRd1lOiJZ",
        "colab_type": "text"
      },
      "source": [
        "## An Example Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDMP_5lwRKdT",
        "colab_type": "text"
      },
      "source": [
        "We train a couple weights and a bias on some randomized data to showcase stochastic gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqh5dQ3iVy4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random as random\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-at47spiqfF",
        "colab_type": "text"
      },
      "source": [
        "### Generate randomized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8ZlPgIsj1Ar",
        "colab_type": "text"
      },
      "source": [
        "This isn't important to understanding gradient descent. All that needs to be known is that this function generates some labeled/truthed data which we can perform gradient descent on, but if you care to read further, feel free.\n",
        "\n",
        "We have two features for each segment of data: $x_1$ and $x_2$.    \n",
        "$x_1 = \\{$Some value on the interval $ [0,1)\\} \\,\\,\\,\\,\\,\\, x_2 = w*x_1+b+n*(-1)^c$    \n",
        "Where $n = \\{$Some value randomly sampled from a normal distribution of properties $\\mu$ and $\\sigma\\}$ and, $b$ and $w$ are passed as inputs.    \n",
        "We then compute $m$ of these segments to generate an entire set of randomized data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUWippAcVzeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generates randomized data according to functions\n",
        "# and given input parameters.\n",
        "def get_random_data(w, b, mu, sigma, m):\n",
        "  data = []\n",
        "  labels = np.random.randint(2, size=m)\n",
        "\n",
        "  for i in range(m):\n",
        "    c = labels[i]\n",
        "    n = np.random.normal(mu, sigma)\n",
        "\n",
        "    # The functions used\n",
        "    x_1 = random.random()\n",
        "    x_2 = w * x_1 + b + ((-1)**c) * n\n",
        "\n",
        "    data.append([x_1, x_2])\n",
        "  \n",
        "  data = np.array(data)\n",
        "  return data, labels\n",
        "\n",
        "# The input parameters for the generated data.\n",
        "truth_weight = 40\n",
        "truth_bias = 30\n",
        "truth_mu = 25\n",
        "truth_sigma = 7\n",
        "truth_m = 10000\n",
        "\n",
        "# Generate the randomized data.\n",
        "data, labels =  get_random_data(truth_weight, truth_bias, truth_mu, truth_sigma, truth_m)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrK3Zon5c4rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Divide up the data into testing and training sets.\n",
        "training_data = data[:7999]\n",
        "training_labels = labels[:7999]\n",
        "testing_data = data[8000:]\n",
        "testing_labels = labels[8000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mzf_nHYFHrCk"
      },
      "source": [
        "### Stochastic Gradient Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxEFzhuHEHPF",
        "colab_type": "text"
      },
      "source": [
        "##### Sigmoid function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM9ui3J4Fos8",
        "colab_type": "text"
      },
      "source": [
        "The sigmoid function is the loss function used in stochastic gradient logistic regression. It is defined as $\\sigma(z)=\\frac{1}{1+e^{-z}}$. It is used to help calculate the gradient magnitude."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL3T_ZZtrnAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performs the sigmoid function.\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbvcL4rsEaes",
        "colab_type": "text"
      },
      "source": [
        "##### Network training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKXXmveHFjDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Network input parameters.\n",
        "epochs = 250\n",
        "lr = 0.01\n",
        "\n",
        "# Initialization of network weights and bias.\n",
        "w_1 = np.random.randn(1)\n",
        "w_2 = np.random.randn(1)\n",
        "b_1 = np.zeros(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyoeDWoEIOLK",
        "colab_type": "text"
      },
      "source": [
        "In order to train the network, some steps need to be taken. We iterate through each epoch, continually updating the weights and the bias through stochastic gradient descent.    \n",
        "    \n",
        "Gradient descent in the code here, includes three important formulas.    \n",
        "First, the $y_p = w_1 * x_1 + w_2 * x_2 + b_1$ formula is realized to find a prediction given certain input values $x_1$ and $x_2$. Once calculated, $y_p$ is used as input to the sigmoid function $a = \\sigma(z)$.    \n",
        "Next, $a$ is used as input to gradient descent formulas such as $g_{w1} = (a - y) * x_1$.    \n",
        "Lastly, gradients are used as input to 'descent' formulas such as $w_1 = w_1 - LR * g_{w1}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96zhsNynrfTv",
        "colab_type": "code",
        "outputId": "f4e28b37-3e69-4f96-baf9-60084cb0f06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training a stochastic gradient logistic regression model.\n",
        "for epoch in np.arange(epochs):\n",
        "    print(\"Epoch \" + str(epoch+1) + \"/\" + str(epochs))\n",
        "\n",
        "    # Iterate through all features.\n",
        "    for i in np.arange(len(training_data)):\n",
        "        # Calculate predictions.\n",
        "        y_pred = w_1 * training_data[i][0] + w_2 * training_data[i][1] + b_1\n",
        "        a = sigmoid(y_pred)\n",
        "        \n",
        "        # Perform gradient descent.\n",
        "        grad_w_1 = (a - training_labels[i]) * training_data[i][0]\n",
        "        grad_w_2 = (a - training_labels[i]) * training_data[i][1]\n",
        "        grad_b_1 = (a - training_labels[i])\n",
        "        \n",
        "        # Apply to weights and bias.\n",
        "        w_1 -= lr * grad_w_1\n",
        "        w_2 -= lr * grad_w_2\n",
        "        b_1 -= lr * grad_b_1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "Epoch 2/250\n",
            "Epoch 3/250\n",
            "Epoch 4/250\n",
            "Epoch 5/250\n",
            "Epoch 6/250\n",
            "Epoch 7/250\n",
            "Epoch 8/250\n",
            "Epoch 9/250\n",
            "Epoch 10/250\n",
            "Epoch 11/250\n",
            "Epoch 12/250\n",
            "Epoch 13/250\n",
            "Epoch 14/250\n",
            "Epoch 15/250\n",
            "Epoch 16/250\n",
            "Epoch 17/250\n",
            "Epoch 18/250\n",
            "Epoch 19/250\n",
            "Epoch 20/250\n",
            "Epoch 21/250\n",
            "Epoch 22/250\n",
            "Epoch 23/250\n",
            "Epoch 24/250\n",
            "Epoch 25/250\n",
            "Epoch 26/250\n",
            "Epoch 27/250\n",
            "Epoch 28/250\n",
            "Epoch 29/250\n",
            "Epoch 30/250\n",
            "Epoch 31/250\n",
            "Epoch 32/250\n",
            "Epoch 33/250\n",
            "Epoch 34/250\n",
            "Epoch 35/250\n",
            "Epoch 36/250\n",
            "Epoch 37/250\n",
            "Epoch 38/250\n",
            "Epoch 39/250\n",
            "Epoch 40/250\n",
            "Epoch 41/250\n",
            "Epoch 42/250\n",
            "Epoch 43/250\n",
            "Epoch 44/250\n",
            "Epoch 45/250\n",
            "Epoch 46/250\n",
            "Epoch 47/250\n",
            "Epoch 48/250\n",
            "Epoch 49/250\n",
            "Epoch 50/250\n",
            "Epoch 51/250\n",
            "Epoch 52/250\n",
            "Epoch 53/250\n",
            "Epoch 54/250\n",
            "Epoch 55/250\n",
            "Epoch 56/250\n",
            "Epoch 57/250\n",
            "Epoch 58/250\n",
            "Epoch 59/250\n",
            "Epoch 60/250\n",
            "Epoch 61/250\n",
            "Epoch 62/250\n",
            "Epoch 63/250\n",
            "Epoch 64/250\n",
            "Epoch 65/250\n",
            "Epoch 66/250\n",
            "Epoch 67/250\n",
            "Epoch 68/250\n",
            "Epoch 69/250\n",
            "Epoch 70/250\n",
            "Epoch 71/250\n",
            "Epoch 72/250\n",
            "Epoch 73/250\n",
            "Epoch 74/250\n",
            "Epoch 75/250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd9obYpDlimC",
        "colab_type": "text"
      },
      "source": [
        "# **Building a model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsAnjJX2TW5Q",
        "colab_type": "text"
      },
      "source": [
        "## How do you build a model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-xcq5mmyf_",
        "colab_type": "text"
      },
      "source": [
        "In Deep Learning, models can be made to perform specific tasks. As mentioned before, we can use convnets for image classification, text prediction, NLP, among other things, but in order to get a model that performs a specific task, we have to do two core things: design the network and train it.\n",
        "\n",
        "Let's say we want a network that can tell if a picture is of a cat or a dog. Since we only have two classes, we want a model that can perform binary classification. A model has two main parts: the conv base and the classifier. \n",
        "\n",
        "The conv base takes the given input (an image of a dog or cat in this scenario) and transforms/filters it in various ways. Each transformation is abstracted into something called a layer. These layers are chosen and placed to extract the features of the input that are important for our use case and update values called weights which fit our model to a given set of data through training. For example, we may want to reduce the entropy of our image for better and faster processing so we use some number of convolutional layers to achieve that end. Although, even Machine Learning experts largely use intuition when designing their own models because Deep Learning is very much like a black box. It's hard to know if a design will classify to the accuracy you need without experimenting with different layer types and combinations.\n",
        "\n",
        "The classifier takes the processed input from the conv base and reduces it to our desired output. In this case, a single number that indicates if the input image contains a cat or a dog. The classifier is also abstracted in the form of layers but uses largely different layers than the conv base. Some notable ones include Flatten, Dense, Dropout, Batch Normalization, Softmax, Sigmoid, and ReLU.\n",
        "\n",
        "Designing a network is a lot of work, but thankfully, network designs can be generalized and used for many different, but similar tasks. These are called pre-trained networks. For instance, there are many different models for image classification and they all work quite well at classifying just about any object in any image; therefore building a network to classify pictures of dogs and cats is quite trivial. We simply choose from one of the well known image classification networks such as Xception, freeze some layers, train some others on images of dogs and cats and it's basically done, though we will get into that a bit later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "het5s6OPTe9q",
        "colab_type": "text"
      },
      "source": [
        "## Building a model in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GSs_fzUTn9o",
        "colab_type": "text"
      },
      "source": [
        "There are different frameworks one can use to build and train a neural network, but we will use the most popular one named Tensorflow-Keras or Keras for short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ48W5T9rHWu",
        "colab_type": "text"
      },
      "source": [
        "#### Load the convolutional base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2O8OMpZT6-Y",
        "colab_type": "text"
      },
      "source": [
        "For our example, we wish to classify images as having either a cat or a dog, thus we use an image classification, pretrained network called Xception. Keras makes this very easy, we just use the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0FgtANCXm_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.xception import Xception\n",
        "\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03gAYe7wVGU3",
        "colab_type": "text"
      },
      "source": [
        "The pretrained network has already been trained on the ImageNet dataset, meaning it already has a lot of information on images we can use. There is more information on this in the \"Finetuning a pretrained model\" section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FLU1cpcVMl3",
        "colab_type": "text"
      },
      "source": [
        "Here are the layers in Xception. Notice how our output is much larger than the single value it needs to be. We learn how to remedy this in the \"Finetuning a pretrained model\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6gZSeAjF7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eJYjA3dlwy8",
        "colab_type": "text"
      },
      "source": [
        "# **Finetuning a pretrained model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-QXCUQbIkI1",
        "colab_type": "text"
      },
      "source": [
        "Training works a bit differently with pretrained models. Pretrained models are frequently trained on large, generalized datasets and released for public use. For example, most image-based pretrained models like Xception are trained on ImageNet--the largest and most representative general image dataset we have to date. This is great for users of pretrained networks because by default, pretrained networks already have a generalized understanding of image classification.\n",
        "\n",
        "In trained models, the deeper into the network you go, the more precise it gets. Conversely, at the top of the network, the more general it is. The first layers of a network hold many abstract concepts of images which are useful and shouldn't be changed such as edges and shapes, therefore when training pretrained networks, we freeze the weights of the first layers. This improves regularization and classification accuracy. Of course if no weights change, training does nothing and we don't need to classify many objects as in the case of ImageNet, we only need to classify cats and dogs, so it is ok and good for the last layers of a network to be trained.\n",
        "\n",
        "Pretrained models are normally set to output values that are different than what we need. As we mentioned before, we only need a binary classifier for classifying cats and dogs. That is, an output of a single value. For example, Xception was trained on ImageNet which has two hundred different classes, but we only need to output two. Thus, we switch out the classifer of a pretrained network with another classifier built for our use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsP97h8Waf9v",
        "colab_type": "text"
      },
      "source": [
        "## Finetune our example network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6OBHOO9q1ou",
        "colab_type": "text"
      },
      "source": [
        "#### Freeze parts of the convolutional base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwkVMx8N8qMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name[:7] == 'block12' or layer.name == 'add_131':\n",
        "    set_trainable = True\n",
        "  \n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfx6PqhPrb7Q",
        "colab_type": "text"
      },
      "source": [
        "#### Concatenate the convolutional base and our classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_3yD1Gat8N",
        "colab_type": "text"
      },
      "source": [
        "Notice how we are able to reduce our output to a single value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUpmocDAO3xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDGYU1TSlpXz",
        "colab_type": "text"
      },
      "source": [
        "# **Compiling a model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AOVgcmEyyeL",
        "colab_type": "text"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXYrtsPKzATi",
        "colab_type": "text"
      },
      "source": [
        "In Gradient Descent we explained how we try to find the optimal weights given a set of data by continually updating our weights bit by bit, but how do we determine how much to update our weights for each iteration? We do this with Optimizers. Optimizers perform varying calculations to try to _optimize_ gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sOiN_C0y7Vu",
        "colab_type": "text"
      },
      "source": [
        "## Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTi_1x1n6UgW",
        "colab_type": "text"
      },
      "source": [
        "In order to understand optimizers, one needs to understand what the Learning Rate is. When performing gradient descent, there exists a gradient magnitude that is used to help determine where the next point in the descent will be, but in order to choose that point, the learning rate is multiplied with the gradient magnitude. Thus, the learning rate determines how much we descend in gradient descent.\n",
        "\n",
        "It's important to be careful and choose a good learning rate because if we choose one that's too large then the network will constantly bounce around the bottom of the gradient, but if we choose one that's too small, the network may never reach the bottom of the gradient and as mentioned before, the bottom of the gradient, gives us the most optimal weight."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLw8JMe18f-P",
        "colab_type": "text"
      },
      "source": [
        "## Modern Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNM56QEf6ToS",
        "colab_type": "text"
      },
      "source": [
        "There are two important, modern optimizers: RMSprop and Adam. There are more but we will only talk about these two.\n",
        "\n",
        "In RMSprop, different weights can have different learning rates, but they are limited by a window and therefore are not only bound by the gradient momentum.\n",
        "\n",
        "In Adam, past gradients are used to optimize for the current one and it implements the idea of momentum.\n",
        "\n",
        "Both are very good, but different optimizers and they should be trialed in image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ73jzMLXAj_",
        "colab_type": "text"
      },
      "source": [
        "## Configuring an Optimizer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRidrNESYh3k",
        "colab_type": "text"
      },
      "source": [
        "Keras makes setting an optimizer really easy for us. We simply set our model to RMSprop for this example and our learning rate to $0.00002$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcZdkSBFW6aK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "# compile model\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    optimizer=optimizers.RMSprop(lr=2e-5), \n",
        "    metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9rJJHNyluEK",
        "colab_type": "text"
      },
      "source": [
        "# **Training a model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhbz9A7ymQao",
        "colab_type": "text"
      },
      "source": [
        "In order to get a model to perform a specific task, we have to train it. When you train a model, you pass the model example data it can use to learn from and output the result you want. For example, we wish for our model to classify a given input image as having a dog or a cat, thus we train a model we've built with as many images as we can of cats and dogs, but we also have to tell the model what image has what classification so it knows how to update the weights. This data is called _truthed_ data. The data we feed it will have labels which indicate which images have dogs and which images have cats.\n",
        "\n",
        "After training a model and building it correctly, we should be able to pass in new images to test how well it is able to do the desired task. This part is often called _validation_ and helpfully most neural network frameworks do this incrementally and automatically as the model is trained. Like our training data, in order to vaidate, we must have input images and their truth, thus before training, it is recommended to take all truthed data and divide it up with 80% going toward training the network and 20% toward validating the network. This way we aren't testing the network based on data it has trained on which would give us _extremely_ warped results.\n",
        "\n",
        "However, there is a problem here. A model is only good as the data it is fed. If we train and evaluate the model on images that are not representative of its final, production use case or conform to that specific data too much, then our model isn't as good as it can be. This is called _overfitting_.\n",
        "\n",
        "If we wish to reduce overfitting (aka regularize), there are a few things we can do. An obvious one is get truthed data as respresentative of the final use case as possible. This means a random sampling of potential inputs and labeling every one of those. If this is not possible, there is something called data augmentation. In data augmentation, we use existing data to create new data to train with. For example, we could reflect all cat and dog images on the y-axis. That would give us more data to train with and hopefully help us get closer to the production use case. Another way to regularize is to change the design of the model. There are types of layers that regularize better than others so those could be used instead. It has also been shown that reducing the complexity of a network, improves regularization.\n",
        "\n",
        "Conversely, it's also important to not underfit. You don't want a model to generalize so much that it ruins its accuracy. This occurs with regularization so it's important to note there's a balance. Validation accuracy will decrease, but regularization will increase. A lot of this is subjective. Mess around with the parameters, try different things, gain intuition, and choose the configuration that seems to work best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKPGI-MjKGpP",
        "colab_type": "text"
      },
      "source": [
        "## Training a pretrained network with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVxEyzq2Y4-q",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare truthed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEhF1ntbY_L4",
        "colab_type": "text"
      },
      "source": [
        "We download a truthed dataset of cat and dog images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWjprHEXJ5Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvoHtdA-K6Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgEKyrqWZHpa",
        "colab_type": "text"
      },
      "source": [
        "We read the image data into Python and divide up the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL8ikM89LlsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJZknXZMZYH0",
        "colab_type": "text"
      },
      "source": [
        "Perform some preprocessing and normalization for Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJvnUK3rXqac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255, \n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEiJOgBUZfMp",
        "colab_type": "text"
      },
      "source": [
        "#### Train the pretrained network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aNR3dk9Zngh",
        "colab_type": "text"
      },
      "source": [
        "We train for 30 epochs in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfAQlC2Oi41L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPih0jvFZuEo",
        "colab_type": "text"
      },
      "source": [
        "#### Display results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcbKgdbtbDxz",
        "colab_type": "text"
      },
      "source": [
        "Let's check out how well our model classifies things."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybWwdzz9bwuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# training and validation accuracy\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='validation acc')\n",
        "plt.title('training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# training and validation loss\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
        "plt.title('training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he3Nw6TOJwjx",
        "colab_type": "text"
      },
      "source": [
        "#### Print out validation loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVQtrk7uJdhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loss, val_acc = model.evaluate_generator(validation_generator, steps=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdddLi1SUMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Validation loss:\", val_loss)\n",
        "print(\"Validation accuracy:\", val_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt7XCsmLDlgj",
        "colab_type": "text"
      },
      "source": [
        "# **Citations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXh0FR_rDnRm",
        "colab_type": "text"
      },
      "source": [
        "- https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence\n",
        "- https://courses.cs.washington.edu/courses/csep573/03wi/lectures/slides/class1.pdf\n",
        "- https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_slides.pdf\n",
        "- https://github.com/nabakin/machine-learning-course/blob/master/HW_3/HW_3.ipynb\n",
        "- https://github.com/nabakin/machine-learning-course/blob/master/HW_4/HW_4_2.ipynb\n",
        "- https://algorithmia.com/blog/introduction-to-optimizers\n",
        "- https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_slides.pdf\n"
      ]
    }
  ]
}